{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: slack-sdk in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (3.34.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install slack-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk.webhook import WebhookClient\n",
    "url=\"https://hooks.slack.com/services/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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slacknotification(message):\n",
    "    webhook = WebhookClient(url)\n",
    "    response = webhook.send(data=message)\n",
    "    if response.status_code == 200 and response.body == \"ok\":\n",
    "        print(\"Slack notification sent successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send Slack notification. Response: {response.body}\")\n",
    "\n",
    "# Take input from the user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slack notification sent successfully!\n"
     ]
    }
   ],
   "source": [
    "from slack_sdk.webhook import WebhookClient\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 1: Generate synthetic user data\n",
    "def generate_user_data(num_users=500):\n",
    "    user_data = []\n",
    "    activities = {\n",
    "        'amenities': ['pool', 'spa', 'gym', 'tennis_court', 'business_center'],\n",
    "        'dining': ['main_restaurant', 'cafe', 'bar', 'room_service', 'buffet'],\n",
    "        'activities': ['city_tour', 'beach_activity', 'cooking_class', 'yoga', 'golf']\n",
    "    }\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        for category in activities:\n",
    "            for activity in activities[category]:\n",
    "                if random.random() < 0.3:  # 30% chance of interaction\n",
    "                    user_data.append({\n",
    "                        'user_id': f'{user_id}',\n",
    "                        'category': category,\n",
    "                        'activity': activity,\n",
    "                        'rating': random.randint(1, 5),\n",
    "                        'review': f\"User {user_id} enjoyed {activity} and rated it {random.randint(1, 5)}.\"\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(user_data)\n",
    "\n",
    "\n",
    "# Step 2: Build user profiles and compute similarity matrix\n",
    "def build_user_profiles(data):\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    similarity_matrix = cosine_similarity(user_profiles)\n",
    "    return similarity_matrix, user_profiles\n",
    "\n",
    "\n",
    "# Step 3: Find similar users\n",
    "def get_similar_users(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, _ = build_user_profiles(data)\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    user_idx = user_profiles.index.get_loc(user_id)\n",
    "    user_similarities = similarity_matrix[user_idx]\n",
    "    similar_user_indices = user_similarities.argsort()[::-1][1:n + 1]\n",
    "    similar_users = user_profiles.index[similar_user_indices]\n",
    "    return list(similar_users)\n",
    "\n",
    "\n",
    "# Step 4: Generate recommendations\n",
    "def get_recommendations(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, _ = build_user_profiles(data)\n",
    "    similar_users = get_similar_users(data, user_id, similarity_matrix=similarity_matrix)\n",
    "    similar_users_data = data[data['user_id'].isin(similar_users)]\n",
    "    recommendations = similar_users_data.groupby('activity').agg({\n",
    "        'rating': 'mean'\n",
    "    }).sort_values('rating', ascending=False)\n",
    "    user_activities = set(data[data['user_id'] == user_id]['activity'])\n",
    "    new_activities = recommendations[~recommendations.index.isin(user_activities)]\n",
    "    return new_activities.head(n), similar_users_data\n",
    "\n",
    "\n",
    "# Step 5: Groq sentiment analysis and suggestions\n",
    "def groq_sentiment_and_suggestions(review):\n",
    "    # Mock for integration (replace with actual Groq API call)\n",
    "    sentiment = \"Positive\" if \"enjoyed\" in review else \"Negative\"\n",
    "    areas_mentioned = [\"Dining\", \"Activities\"]  # Mock example\n",
    "    suggestions = {\n",
    "        \"Dining\": \"Improve service speed during peak hours.\",\n",
    "        \"Activities\": \"Introduce additional options like yoga or city tours.\"\n",
    "    }\n",
    "    return sentiment, areas_mentioned, suggestions\n",
    "\n",
    "\n",
    "# Step 6: Format and send Slack notification\n",
    "def send_slack_notification(user_id, recommendations, reviews, webhook_url):\n",
    "    slack_message = f\"*__Recommendations for User {user_id}:__*\\n\"\n",
    "\n",
    "    if recommendations.empty:\n",
    "        slack_message += \"No new recommendations available.\\n\"\n",
    "    else:\n",
    "        for activity, row in recommendations.iterrows():\n",
    "            slack_message += f\"• *{activity}* (Avg. Rating: {row['rating']:.2f})\\n\"\n",
    "            activity_reviews = reviews[reviews['activity'] == activity]\n",
    "\n",
    "            # Add sentiment, areas, and suggestions\n",
    "            for _, review in activity_reviews.iterrows():\n",
    "                sentiment, areas, suggestions = groq_sentiment_and_suggestions(review['review'])\n",
    "                slack_message += f\"  - Review: {review['review']}\\n\"\n",
    "                slack_message += f\"    Sentiment: {sentiment}\\n\"\n",
    "                slack_message += f\"    Areas: {', '.join(areas)}\\n\"\n",
    "                slack_message += \"    Suggestions:\\n\"\n",
    "                for area, suggestion in suggestions.items():\n",
    "                    slack_message += f\"      - {area}: {suggestion}\\n\"\n",
    "\n",
    "    # Send the message to Slack\n",
    "    webhook = WebhookClient(webhook_url)\n",
    "    response = webhook.send(text=slack_message)\n",
    "    if response.status_code == 200 and response.body == \"ok\":\n",
    "        print(\"Slack notification sent successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send Slack notification. Response: {response.body}\")\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    webhook_url = \"https://hooks.slack.com/services/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\"  # Replace with your Slack webhook URL\n",
    "\n",
    "    # Generate synthetic data\n",
    "    data = generate_user_data(num_users=100)\n",
    "\n",
    "    # Get user input\n",
    "    user_id = input(\"Enter your user ID (e.g., '0'): \")\n",
    "\n",
    "    # Generate recommendations and reviews\n",
    "    similarity_matrix, _ = build_user_profiles(data)\n",
    "    recommendations, reviews = get_recommendations(data, user_id, n=10, similarity_matrix=similarity_matrix)\n",
    "\n",
    "    # Send recommendations to Slack\n",
    "    send_slack_notification(user_id, recommendations, reviews, webhook_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (1.60.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 61.4/76.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.5/76.5 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.60.0\n",
      "    Uninstalling openai-1.60.0:\n",
      "      Successfully uninstalled openai-1.60.0\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from openai==0.28) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m review_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Review Text: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Process the review\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m slack_message \u001b[38;5;241m=\u001b[39m process_review(user_id, review_text)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Send the Slack message\u001b[39;00m\n\u001b[0;32m    133\u001b[0m send_slack_message(slack_message)\n",
      "Cell \u001b[1;32mIn[17], line 96\u001b[0m, in \u001b[0;36mprocess_review\u001b[1;34m(user_id, review_text, n)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_review\u001b[39m(user_id, review_text, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m     sentiment_result \u001b[38;5;241m=\u001b[39m sentiment_and_areas(review_text)\n\u001b[0;32m     97\u001b[0m     suggestions \u001b[38;5;241m=\u001b[39m suggestion_generator(sentiment_result, review_text)\n\u001b[0;32m    100\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m get_recommendations(data, user_id, n\u001b[38;5;241m=\u001b[39mn, similarity_matrix\u001b[38;5;241m=\u001b[39msimilarity_matrix)\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36msentiment_and_areas\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Return the sentiment and areas\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m sentiment_and_areas \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_and_areas\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq\n",
    "# Set up your Slack webhook URL\n",
    "SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/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\"  # Replace with your actual Slack webhook URL\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "# Function to perform sentiment analysis and extract areas\n",
    "def sentiment_and_areas(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a sentiment and area extractor. Your task is to analyze the review and determine the sentiment (Positive or Negative). \"\n",
    "                           f\"Additionally, extract the areas mentioned in the review, like 'Room', 'Service', 'Dining', etc. \"\n",
    "                           f\"The response should return a JSON object with 'Sentiment' and 'Areas' as keys.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Review text: '{text}'. Please provide the sentiment (Positive/Negative) and the areas mentioned in the review.\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    \n",
    "    # Return the sentiment and areas\n",
    "    sentiment_and_areas = json.loads(response)\n",
    "    return sentiment_and_areas\n",
    "\n",
    "# Function to provide suggestions based on areas and sentiment\n",
    "def suggestion_generator(Sentiment_dict, text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a suggestion provider who gives suggestions to improve areas mentioned in the review text. \"\n",
    "                           f\"Based on the sentiment analysis, provide actionable suggestions to improve the areas listed.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Provide suggestions to improve the following areas: {Sentiment_dict['Areas']} by analyzing this review: '{text}'\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    suggestion_response = chat_completion.choices[0].message.content.strip()\n",
    "    return suggestion_response\n",
    "\n",
    "# Load Data (Assumed to be available in a CSV file)\n",
    "data = pd.read_csv(\"recommendation_data.csv\")\n",
    "\n",
    "# Step 1: Build User Profiles and Similarity Matrix\n",
    "def build_user_profiles(data):\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    similarity_matrix = cosine_similarity(user_profiles)\n",
    "    return similarity_matrix, user_profiles\n",
    "\n",
    "# Step 2: Find Similar Users\n",
    "def get_similar_users(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    user_idx = user_profiles.index.get_loc(user_id)\n",
    "    user_similarities = similarity_matrix[user_idx]\n",
    "\n",
    "    similar_user_indices = user_similarities.argsort()[::-1][1:n + 1]\n",
    "    similar_users = user_profiles.index[similar_user_indices]\n",
    "\n",
    "    return list(similar_users)\n",
    "\n",
    "# Step 3: Generate Recommendations\n",
    "def get_recommendations(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    similar_users = get_similar_users(data, user_id, n=n, similarity_matrix=similarity_matrix)\n",
    "\n",
    "    similar_users_data = data[data['user_id'].isin(similar_users)]\n",
    "\n",
    "    recommendations = similar_users_data.groupby('activity').agg({'rating': 'mean'}).sort_values('rating', ascending=False)\n",
    "\n",
    "    user_activities = set(data[data['user_id'] == user_id]['activity'])\n",
    "    new_activities = recommendations[~recommendations.index.isin(user_activities)]\n",
    "\n",
    "    return new_activities.head(n)\n",
    "\n",
    "# Step 4: Process Review and Prepare Slack Message\n",
    "def process_review(user_id, review_text, n=5):\n",
    "    sentiment_result = sentiment_and_areas(review_text)\n",
    "    suggestions = suggestion_generator(sentiment_result, review_text)\n",
    "\n",
    "    \n",
    "    recommendations = get_recommendations(data, user_id, n=n, similarity_matrix=similarity_matrix)\n",
    "    recommended_activities = recommendations.index.tolist()\n",
    "\n",
    "    slack_message = {\n",
    "        \"text\": f\"⚠️ *Review Alert for User {user_id}* ⚠️\\n\\n\"\n",
    "                f\"*Review*: {review_text}\\n\"\n",
    "                f\"*Sentiment*: {sentiment_result['Sentiment']}\\n\"\n",
    "                f\"*Areas Mentioned*: {', '.join(sentiment_result['Areas'])}\\n\"\n",
    "                f\"*Suggestions*: {suggestions}\\n\"\n",
    "                f\"*Recommended Activities*: {', '.join(recommended_activities)}\"\n",
    "    }\n",
    "\n",
    "    return slack_message\n",
    "\n",
    "# Step 5: Send Slack Message\n",
    "def send_slack_message(message):\n",
    "    response = requests.post(SLACK_WEBHOOK_URL, data=json.dumps(message), headers={'Content-Type': 'application/json'})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Message sent to Slack successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send message. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 6: Interactive Input (User ID and Review)\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt for user input\n",
    "    user_id = input(\"Enter User ID: \")\n",
    "    review_text = input(\"Enter Review Text: \")\n",
    "\n",
    "    # Process the review\n",
    "    slack_message = process_review(user_id, review_text)\n",
    "\n",
    "    # Send the Slack message\n",
    "    send_slack_message(slack_message)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq\n",
    "# Set up your Slack webhook URL\n",
    "SLACK_WEBHOOK_URL = \"https://hooks.slack.com/services/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\"  # Replace with your actual Slack webhook URL\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "# Function to perform sentiment analysis and extract areas\n",
    "def sentiment_and_areas(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a sentiment and area extractor. Your task is to analyze the review and determine the sentiment (Positive or Negative). \"\n",
    "                           f\"Additionally, extract the areas mentioned in the review, like 'Room', 'Service', 'Dining', etc. \"\n",
    "                           f\"The response should return a JSON object with 'Sentiment' and 'Areas' as keys.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Review text: '{text}'. Please provide the sentiment (Positive/Negative) and the areas mentioned in the review.\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    \n",
    "    # Return the sentiment and areas\n",
    "    sentiment_and_areas =response\n",
    "    return sentiment_and_areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the output in JSON format:\\n\\n```json\\n{\\n  \"Sentiment\": \"Negative\",\\n  \"Areas\": [\"Room\", \"Dining\"]\\n}\\n```\\n\\nIn this review, the sentiment is Negative because the reviewer mentioned that the \"Room\" is \"bad\", and the \"food\" is \"cold\", which are both negative statements. The areas mentioned in the review are \"Room\" and \"Dining\".'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_and_areas(\"Room is bad, food is cols\")\n",
    "#3r ecommenda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to provide suggestions based on areas and sentiment\n",
    "def suggestion_generator(Sentiment_dict, text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a suggestion provider who gives suggestions to improve areas mentioned in the review text. \"\n",
    "                           f\"Based on the sentiment analysis, provide actionable suggestions to improve the areas listed.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Provide suggestions to improve the following areas: {Sentiment_dict['Areas']} by analyzing this review: '{text}'\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    suggestion_response = chat_completion.choices[0].message.content.strip()\n",
    "    return suggestion_response\n",
    "\n",
    "# Load Data (Assumed to be available in a CSV file)\n",
    "data = pd.read_csv(\"Interaction.csv\")\n",
    "\n",
    "# Step 1: Build User Profiles and Similarity Matrix\n",
    "def build_user_profiles(data):\n",
    "    user_profiles = data.pivot_table(index='User_ID', columns='Activity', values='Rating').fillna(0)\n",
    "    similarity_matrix = cosine_similarity(user_profiles)\n",
    "    return similarity_matrix, user_profiles\n",
    "\n",
    "# Step 2: Find Similar Users\n",
    "def get_similar_users(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    user_profiles = data.pivot_table(index='User_ID', columns='Activity', values='Rating').fillna(0)\n",
    "    user_idx = user_profiles.index.get_loc(user_id)\n",
    "    user_similarities = similarity_matrix[user_idx]\n",
    "\n",
    "    similar_user_indices = user_similarities.argsort()[::-1][1:n + 1]\n",
    "    similar_users = user_profiles.index[similar_user_indices]\n",
    "\n",
    "    return list(similar_users)\n",
    "\n",
    "# Step 3: Generate Recommendations\n",
    "def get_recommendations(data, user_id, n=5, similarity_matrix=None):\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    similar_users = get_similar_users(data, user_id, n=5, similarity_matrix=similarity_matrix)\n",
    "\n",
    "    similar_users_data = user_profiles[user_profiles.index.isin(similar_users)]\n",
    "\n",
    "    similar_users_df = similar_users_data.mean()\n",
    "    user_df = user_profiles[user_profiles.index == user_id].T\n",
    "    user_df.columns = ['rating']\n",
    "    unseen_data = user_df[user_df['rating'] == 0]\n",
    "    user_activities = set(data[data['user_id'] == user_id]['activity'])\n",
    "    recommended = recommendations[~recommendations.index.isin(user_activities)]\n",
    "\n",
    "    return similar_users_data\n",
    "    # will change the col names in .csv file?later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Activity</th>\n",
       "      <th>bar</th>\n",
       "      <th>beach_activity</th>\n",
       "      <th>buffet</th>\n",
       "      <th>business_center</th>\n",
       "      <th>cafe</th>\n",
       "      <th>city_tour</th>\n",
       "      <th>cooking_class</th>\n",
       "      <th>golf</th>\n",
       "      <th>gym</th>\n",
       "      <th>main_restaurant</th>\n",
       "      <th>pool</th>\n",
       "      <th>room_service</th>\n",
       "      <th>spa</th>\n",
       "      <th>tennis_court</th>\n",
       "      <th>yoga</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>User_01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_010</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_0100</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_01000</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_010000</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_09995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_09996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_09997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_09998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User_09999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Activity     bar  beach_activity  buffet  business_center  cafe  city_tour  \\\n",
       "User_ID                                                                      \n",
       "User_01      0.0             0.0     0.0              3.0   1.0        2.5   \n",
       "User_010     2.0             0.0     3.0              1.0   5.0        2.0   \n",
       "User_0100    2.0             5.0     3.5              5.0   5.0        0.0   \n",
       "User_01000   1.0             2.0     0.0              3.5   4.0        0.0   \n",
       "User_010000  5.0             0.0     0.0              0.0   3.0        2.0   \n",
       "...          ...             ...     ...              ...   ...        ...   \n",
       "User_09995   0.0             0.0     0.0              0.0   1.0        0.0   \n",
       "User_09996   0.0             5.0     0.0              2.0   0.0        4.0   \n",
       "User_09997   0.0             2.0     4.0              0.0   0.0        0.0   \n",
       "User_09998   0.0             4.5     0.0              3.0   4.0        3.5   \n",
       "User_09999   1.0             0.0     4.0              0.0   3.0        4.0   \n",
       "\n",
       "Activity     cooking_class  golf  gym  main_restaurant  pool  room_service  \\\n",
       "User_ID                                                                      \n",
       "User_01                0.0   3.5  1.0              5.0   0.0           0.0   \n",
       "User_010               0.0   0.0  4.5              3.0   3.5           4.0   \n",
       "User_0100              5.0   0.0  2.0              2.0   0.0           0.0   \n",
       "User_01000             3.0   0.0  1.0              0.0   4.0           5.0   \n",
       "User_010000            0.0   0.0  0.0              0.0   3.0           0.0   \n",
       "...                    ...   ...  ...              ...   ...           ...   \n",
       "User_09995             0.0   3.0  0.0              3.0   0.0           3.0   \n",
       "User_09996             0.0   0.0  3.5              0.0   2.0           3.0   \n",
       "User_09997             0.0   3.5  3.0              0.0   0.0           4.0   \n",
       "User_09998             0.0   3.0  0.0              4.0   4.0           0.0   \n",
       "User_09999             0.0   0.0  4.0              3.0   0.0           3.0   \n",
       "\n",
       "Activity     spa  tennis_court      yoga  \n",
       "User_ID                                   \n",
       "User_01      1.0           1.0  2.666667  \n",
       "User_010     0.0           0.0  2.000000  \n",
       "User_0100    4.0           0.0  0.000000  \n",
       "User_01000   1.5           0.0  0.000000  \n",
       "User_010000  1.0           0.0  2.000000  \n",
       "...          ...           ...       ...  \n",
       "User_09995   0.0           3.0  0.000000  \n",
       "User_09996   0.0           3.0  4.000000  \n",
       "User_09997   0.0           0.0  3.000000  \n",
       "User_09998   1.0           0.0  1.333333  \n",
       "User_09999   0.0           3.0  1.500000  \n",
       "\n",
       "[10000 rows x 15 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "user_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_04824', 'User_07215', 'User_05789', 'User_01063', 'User_09369']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_users(data, \"User_01\")\n",
    "#now from ..when like taking from kaggle . that is sentiment data...im talking about rwc\n",
    "# actully u have to make dataset by missing some of the activities...so we can make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m review_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Review Text: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Process the review\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m slack_message \u001b[38;5;241m=\u001b[39m process_review(user_id, review_text)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Send the Slack message\u001b[39;00m\n\u001b[0;32m     39\u001b[0m send_slack_message(slack_message)\n",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m, in \u001b[0;36mprocess_review\u001b[1;34m(user_id, review_text, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_review\u001b[39m(user_id, review_text, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     sentiment_result \u001b[38;5;241m=\u001b[39m sentiment_and_areas(review_text)\n\u001b[0;32m      3\u001b[0m     suggestions \u001b[38;5;241m=\u001b[39m suggestion_generator(sentiment_result, review_text)\n\u001b[0;32m      6\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m get_recommendations(data, user_id, n\u001b[38;5;241m=\u001b[39mn, similarity_matrix\u001b[38;5;241m=\u001b[39msimilarity_matrix)\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36msentiment_and_areas\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Return the sentiment and areas\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m sentiment_and_areas \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_and_areas\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "def process_review(user_id, review_text, n=5):\n",
    "    sentiment_result = sentiment_and_areas(review_text)\n",
    "    suggestions = suggestion_generator(sentiment_result, review_text)\n",
    "\n",
    "    \n",
    "    recommendations = get_recommendations(data, user_id, n=n, similarity_matrix=similarity_matrix)\n",
    "    recommended_activities = recommendations.index.tolist()\n",
    "\n",
    "    slack_message = {\n",
    "        \"text\": f\"⚠️ *Review Alert for User {user_id}* ⚠️\\n\\n\"\n",
    "                f\"*Review*: {review_text}\\n\"\n",
    "                f\"*Sentiment*: {sentiment_result['Sentiment']}\\n\"\n",
    "                f\"*Areas Mentioned*: {', '.join(sentiment_result['Areas'])}\\n\"\n",
    "                f\"*Suggestions*: {suggestions}\\n\"\n",
    "                f\"*Recommended Activities*: {', '.join(recommended_activities)}\"\n",
    "    }\n",
    "\n",
    "    return slack_message\n",
    "\n",
    "# Step 5: Send Slack Message\n",
    "def send_slack_message(message):\n",
    "    response = requests.post(SLACK_WEBHOOK_URL, data=json.dumps(message), headers={'Content-Type': 'application/json'})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Message sent to Slack successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send message. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 6: Interactive Input (User ID and Review)\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt for user input\n",
    "    user_id = input(\"Enter User ID: \")\n",
    "    review_text = input(\"Enter Review Text: \")\n",
    "\n",
    "    # Process the review\n",
    "    slack_message = process_review(user_id, review_text)\n",
    "\n",
    "    # Send the Slack message\n",
    "    send_slack_message(slack_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\patlo.nanitata_028\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Sentiment and Areas Response: {\"Sentiment\": \"Negative\", \"Areas\": [\"Room\"]}\n",
      "Sentiment and Areas Result: {'Sentiment': 'Negative', 'Areas': ['Room']}\n",
      "Raw Suggestion Generator Response: {'Room': 'Improve housekeeping standards by ensuring the room is thoroughly cleaned and disinfected before guests check-in, and conduct regular room inspections to ensure consistency and quality.'}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m sentiment_areas_result \u001b[38;5;241m=\u001b[39m sentiment_and_areas(text)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment and Areas Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentiment_areas_result)\n\u001b[1;32m---> 78\u001b[0m suggestions \u001b[38;5;241m=\u001b[39m suggestion_generator(Sentiment_dict, text)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggestions Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, suggestions)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Step 1: Build User Profiles and Similarity Matrix\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 71\u001b[0m, in \u001b[0;36msuggestion_generator\u001b[1;34m(Sentiment_dict, text)\u001b[0m\n\u001b[0;32m     69\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Suggestion Generator Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response)\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = pd.read_csv(\"Sentiment_Analysis_data.csv\")\n",
    "activities = {\n",
    "        'amenities': ['pool', 'spa', 'gym', 'tennis_court', 'business_center'],\n",
    "        'dining': ['main_restaurant', 'cafe', 'bar', 'room_service', 'buffet'],\n",
    "        'activities': ['city_tour', 'beach_activity', 'cooking_class', 'yoga', 'golf']\n",
    "    }\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "# Input text\n",
    "text = \"room not cleaned\"\n",
    "\n",
    "# Function to analyze sentiment and extract areas\n",
    "def sentiment_and_areas(text):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a sentiment and area extractor. Your task is to analyze the review and determine the sentiment \"\n",
    "                    \"(Positive or Negative). Additionally, extract the areas mentioned in the review, like 'Room', 'Service', 'Dining', etc. \"\n",
    "                    \"The response must be a valid JSON object in this format: \"\n",
    "                    '{\"Sentiment\": \"Positive or Negative\", \"Areas\": [\"list\", \"of\", \"areas\"]}. '\n",
    "                    \"Do not include any text outside the JSON object.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Review text: '{text}'. Please provide the sentiment (Positive/Negative) and the areas mentioned in the review.\",\n",
    "            },\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    print(\"Raw Sentiment and Areas Response:\", response)  # Debugging\n",
    "    return json.loads(response)\n",
    "\n",
    "# Main execution\n",
    "\n",
    "def suggestion_generator(Sentiment_dict: dict, text: str):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a suggestion provider who gives actionable recommendations to improve areas mentioned in the review. \"\n",
    "                    \"Analyze the review text and provide suggestions in JSON format for each area. \"\n",
    "                    \"The JSON object must follow this schema: {'Area': 'Suggestion'}. \"\n",
    "                    \"Do not include any text outside the JSON object.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Provide suggestions to improve the following areas: {Sentiment_dict['Areas']} based on the review: '{text}'.\",\n",
    "            },\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    # Extract and parse the JSON response\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    print(\"Raw Suggestion Generator Response:\", response)  # Debugging\n",
    "    return json.loads(response)\n",
    "\n",
    "# Example Execution\n",
    "Sentiment_dict = {\"Sentiment\": \"Negative\", \"Areas\": [\"Room\"]}\n",
    "text = \"room not cleaned\"\n",
    "sentiment_areas_result = sentiment_and_areas(text)\n",
    "print(\"Sentiment and Areas Result:\", sentiment_areas_result)\n",
    "suggestions = suggestion_generator(Sentiment_dict, text)\n",
    "print(\"Suggestions Result:\", suggestions)\n",
    "\n",
    "\n",
    "# Step 1: Build User Profiles and Similarity Matrix\n",
    "def build_user_profiles(data):\n",
    "    \"\"\"\n",
    "    Create a pivot table for user profiles and calculate cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing 'user_id', 'activity', and 'rating'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (similarity_matrix, user_profiles)\n",
    "    \"\"\"\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    similarity_matrix = cosine_similarity(user_profiles)\n",
    "    return similarity_matrix, user_profiles\n",
    "\n",
    "# Step 2: Find Similar Users\n",
    "def get_similar_users(data, user_id, n=4, similarity_matrix=None):\n",
    "    \"\"\"\n",
    "    Find the top N most similar users to a given user based on the similarity matrix.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing 'user_id', 'activity', and 'rating'.\n",
    "        user_id (int): The user ID to find similar users for.\n",
    "        n (int): Number of similar users to retrieve.\n",
    "        similarity_matrix (ndarray): Precomputed similarity matrix (optional).\n",
    "\n",
    "    Returns:\n",
    "        list: User IDs of the top N similar users.\n",
    "    \"\"\"\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    # Get user index from profiles\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    user_idx = user_profiles.index.get_loc(user_id)\n",
    "\n",
    "    # Get similarity scores and find top N similar users\n",
    "    user_similarities = similarity_matrix[user_idx]\n",
    "    similar_user_indices = user_similarities.argsort()[::-1][1:n + 1]  # Exclude the user themselves\n",
    "    similar_users = user_profiles.index[similar_user_indices]\n",
    "\n",
    "    return list(similar_users)\n",
    "activities = {\n",
    "        'amenities': ['pool', 'spa', 'gym', 'tennis_court', 'business_center'],\n",
    "        'dining': ['main_restaurant', 'cafe', 'bar', 'room_service', 'buffet'],\n",
    "        'activities': ['city_tour', 'beach_activity', 'cooking_class', 'yoga', 'golf']\n",
    "    }\n",
    "# Step 3: Generate Recommendations\n",
    "def get_recommendations(data, user_id, n=10, similarity_matrix=None):\n",
    "    \"\"\"\n",
    "    Generate activity recommendations for a given user based on similar users' preferences.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data containing 'user_id', 'activity', and 'rating'.\n",
    "        user_id (int): The user ID to generate recommendations for.\n",
    "        n (int): Number of recommendations to retrieve.\n",
    "        similarity_matrix (ndarray): Precomputed similarity matrix (optional).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Top N recommended activities with their mean ratings.\n",
    "    \"\"\"\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    # Get similar users\n",
    "    similar_users = get_similar_users(data, user_id, n=4, similarity_matrix=similarity_matrix)\n",
    "\n",
    "    # Filter data to include only similar users\n",
    "    similar_users_data = data[data['user_id'].isin(similar_users)]\n",
    "\n",
    "    # Aggregate mean ratings for each activity\n",
    "    recommendations = similar_users_data.groupby('activity').agg({'rating': 'mean'}).sort_values('rating', ascending=False)\n",
    "\n",
    "    # Exclude activities already rated by the user\n",
    "    user_activities = set(data[data['user_id'] == user_id]['activity'])\n",
    "    new_activities = recommendations[~recommendations.index.isin(user_activities)]\n",
    "\n",
    "    return new_activities.head(n)\n",
    "\n",
    "# Example Usage\n",
    "recommendations = get_recommendations(data, user_id=123, n=4)\n",
    "recommended_activities = recommendations.index.tolist()\n",
    "print(f\"*Recommended Activities*: {', '.join(recommended_activities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Check if user exists in the dataset\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Generate recommendations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\patlo.NANITATA_028\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "data = pd.read_csv(\"Sentiment_Analysis_data.csv\")\n",
    "activities = {\n",
    "        'amenities': ['pool', 'spa', 'gym', 'tennis_court', 'business_center'],\n",
    "        'dining': ['main_restaurant', 'cafe', 'bar', 'room_service', 'buffet'],\n",
    "        'activities': ['city_tour', 'beach_activity', 'cooking_class', 'yoga', 'golf']\n",
    "    }\n",
    "# Step 1: Build User Profiles and Similarity Matrix\n",
    "def build_user_profiles(data):\n",
    "    \"\"\"\n",
    "    Create a pivot table for user profiles and calculate cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset with columns ['user_id', 'activity', 'rating'].\n",
    "\n",
    "    Returns:\n",
    "        tuple: (similarity_matrix, user_profiles)\n",
    "    \"\"\"\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    similarity_matrix = cosine_similarity(user_profiles)\n",
    "    return similarity_matrix, user_profiles\n",
    "\n",
    "# Step 2: Find Similar Users\n",
    "def get_similar_users(data, user_id, n=5, similarity_matrix=None):\n",
    "    \"\"\"\n",
    "    Find the top N similar users to the input user based on similarity matrix.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset with columns ['user_id', 'activity', 'rating'].\n",
    "        user_id (int): ID of the user to find similar users for.\n",
    "        n (int): Number of similar users to retrieve.\n",
    "        similarity_matrix (ndarray): Precomputed similarity matrix.\n",
    "\n",
    "    Returns:\n",
    "        list: List of user IDs of similar users.\n",
    "    \"\"\"\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    user_profiles = data.pivot_table(index='user_id', columns='activity', values='rating').fillna(0)\n",
    "    user_idx = user_profiles.index.get_loc(user_id)\n",
    "    user_similarities = similarity_matrix[user_idx]\n",
    "\n",
    "    # Get indices of the top `n` similar users (excluding the user itself)\n",
    "    similar_user_indices = user_similarities.argsort()[::-1][1:n + 1]\n",
    "    similar_users = user_profiles.index[similar_user_indices]\n",
    "\n",
    "    return list(similar_users)\n",
    "\n",
    "# Step 3: Generate Recommendations\n",
    "def get_recommendations(data, user_id, n=5, similarity_matrix=None):\n",
    "    \"\"\"\n",
    "    Generate recommendations for the user based on similar users' activities.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset with columns ['user_id', 'activity', 'rating'].\n",
    "        user_id (int): ID of the user to generate recommendations for.\n",
    "        n (int): Number of recommendations to retrieve.\n",
    "        similarity_matrix (ndarray): Precomputed similarity matrix.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Recommendations with average ratings.\n",
    "    \"\"\"\n",
    "    if similarity_matrix is None:\n",
    "        similarity_matrix, user_profiles = build_user_profiles(data)\n",
    "\n",
    "    similar_users = get_similar_users(data, user_id, n=5, similarity_matrix=similarity_matrix)\n",
    "\n",
    "    # Filter activities rated by similar users\n",
    "    similar_users_data = data[data['user_id'].isin(similar_users)]\n",
    "\n",
    "    # Calculate average ratings for activities\n",
    "    recommendations = similar_users_data.groupby('activity').agg({\n",
    "        'rating': 'mean'\n",
    "    }).sort_values('rating', ascending=False)\n",
    "\n",
    "    # Filter out activities the user has already tried\n",
    "    user_activities = set(data[data['user_id'] == user_id]['activity'])\n",
    "    new_activities = recommendations[~recommendations.index.isin(user_activities)]\n",
    "\n",
    "    return new_activities.head(n)\n",
    "\n",
    "# Step 4: User Interaction\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"Sentiment_Analysis_data.csv\")\n",
    "    activities = {\n",
    "        'amenities': ['pool', 'spa', 'gym', 'tennis_court', 'business_center'],\n",
    "        'dining': ['main_restaurant', 'cafe', 'bar', 'room_service', 'buffet'],\n",
    "        'activities': ['city_tour', 'beach_activity', 'cooking_class', 'yoga', 'golf']\n",
    "    }\n",
    "    user_id = 1\n",
    "    # Check if user exists in the dataset\n",
    "    if user_id not in data['user_id'].unique():\n",
    "        print(f\"User {user_id} not found in the dataset.\")\n",
    "    else:\n",
    "        # Generate recommendations\n",
    "        similarity_matrix, _ = build_user_profiles(data)\n",
    "        recommendations = get_recommendations(data, user_id, n=5, similarity_matrix=similarity_matrix)\n",
    "\n",
    "        # Output recommendations\n",
    "        if recommendations.empty:\n",
    "            print(f\"No new recommendations available for User {user_id}.\")\n",
    "        else:\n",
    "            print(f\"Recommendations for User {user_id}:\")\n",
    "            print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
